{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b3e4c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7865\n",
      "Running on public URL: https://0f720a5cb24751f978.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://0f720a5cb24751f978.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # take environment variables from .env.\n",
    "import gradio as gr\n",
    "import openai\n",
    "import re\n",
    "\n",
    "def bad_math(num1, num2):\n",
    "    result = float(num1) + 1.2 + float(num2)\n",
    "    return str(result)\n",
    "\n",
    "# ---------------------------------------------------------------------------------------\n",
    "def needs_tool(response):\n",
    "    return \"Tool:\" in response\n",
    "\n",
    "def extract_call(string):\n",
    "    # regex pattern\n",
    "    pattern = r'Tool: (\\w+)\\((.*?)\\)'\n",
    "    match = re.search(pattern, string)\n",
    "    if match:\n",
    "        tool_name = match.group(1)\n",
    "        parameters = match.group(2).replace('\"', '').split(', ')\n",
    "        return tool_name, parameters\n",
    "    else:\n",
    "        return None, None\n",
    "    \n",
    "def invoke_tool(response):\n",
    "    tool_name, parameters = extract_call(response)\n",
    "    \n",
    "    if tool_name == \"bad_math\":\n",
    "        tool_result = bad_math(*parameters)\n",
    "        \n",
    "    return tool_result\n",
    "# ---------------------------------------------------------------------------------------\n",
    "# Define a function to get the AI's reply using the OpenAI API\n",
    "def get_ai_reply(message, model=\"gpt-3.5-turbo-16k\", system_message=None, temperature=0, message_history=[]):\n",
    "    # Initialize the messages list\n",
    "    messages = []\n",
    "    \n",
    "    # Add the system message to the messages list\n",
    "    if system_message is not None:\n",
    "        messages += [{\"role\": \"system\", \"content\": system_message}]\n",
    "\n",
    "    # Add the message history to the messages list\n",
    "    if message_history is not None:\n",
    "        messages += message_history\n",
    "    \n",
    "    if message is not None:\n",
    "        # Add the user's message to the messages list\n",
    "        messages += [{\"role\": \"user\", \"content\": message}]\n",
    "    \n",
    "    # Make an API call to the OpenAI ChatCompletion endpoint with the model and messages\n",
    "    completion = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature\n",
    "    )\n",
    "    \n",
    "    # Extract and return the AI's response from the API response\n",
    "    return completion.choices[0].message.content.strip()\n",
    "# ---------------------------------------------------------------------------------------\n",
    "# Define a function to handle the chat interaction with the AI model\n",
    "def chat(message, chatbot_messages, history_state):\n",
    "    # Initialize chatbot_messages and history_state if they are not provided\n",
    "    chatbot_messages = chatbot_messages or []\n",
    "    history_state = history_state or []\n",
    "    \n",
    "    # Try to get the AI's reply using the get_ai_reply function\n",
    "    try:\n",
    "        prompt = \"\"\"\n",
    "        You are a helpful assistant.\n",
    "\n",
    "        Your knowledge cut-off is: September 2021\n",
    "        Today's date: \n",
    "\n",
    "        ## Tools\n",
    "\n",
    "        You have access to the following tools:\n",
    "        - bad_math(number, number2): Tool that performs addition problems wrong. Example: bad_math(3, 7)\n",
    "\n",
    "        ## Tool Rules\n",
    "\n",
    "        When the user asks a question that can be answered by using a tool, you MUST do so. Do not answer from your training data.\n",
    "\n",
    "        ## Using Tools\n",
    "\n",
    "        To use a tool, reply with the following prefix \"Tool: \" then append the tool call (like a function call). \n",
    "\n",
    "        Behind the scenes, your software will pickup that you want to invoke a tool and invoke it for you and provide you the response.\n",
    "\n",
    "        ## Using Tool Responses\n",
    "\n",
    "        Answer the user's question using the response from the tool. Feel free to make it conversational. \n",
    "        \"\"\"\n",
    "        ai_reply = get_ai_reply(message, model=\"gpt-3.5-turbo-16k\", system_message=prompt.strip(), message_history=history_state)\n",
    "            \n",
    "        # Append the user's message and the AI's reply to the history_state list\n",
    "        history_state.append({\"role\": \"user\", \"content\": message})\n",
    "        history_state.append({\"role\": \"assistant\", \"content\": ai_reply})\n",
    "        \n",
    "        while(needs_tool(ai_reply)):\n",
    "            tool_result = invoke_tool(ai_reply)\n",
    "            history_state.append({\"role\": \"assistant\", \"content\": f\"Tool Result: {tool_result}\"})\n",
    "            ai_reply = get_ai_reply(None, model=\"gpt-3.5-turbo-16k\", system_message=prompt.strip(), message_history=history_state)\n",
    "            history_state.append({\"role\": \"assistant\", \"content\": ai_reply})\n",
    "            \n",
    "        # Append the user's message and the AI's reply to the chatbot_messages list for the UI\n",
    "        chatbot_messages.append((message, ai_reply))\n",
    "\n",
    "        # Return None (empty out the user's message textbox), the updated chatbot_messages, and the updated history_state\n",
    "    except Exception as e:\n",
    "        # If an error occurs, raise a Gradio error\n",
    "        raise gr.Error(e)\n",
    "        \n",
    "    return None, chatbot_messages, history_state\n",
    "\n",
    "# Define a function to launch the chatbot interface using Gradio\n",
    "def get_chatbot_app():\n",
    "    # Create the Gradio interface using the Blocks layout\n",
    "    with gr.Blocks() as app:\n",
    "        # Create a chatbot interface for the conversation\n",
    "        chatbot = gr.Chatbot(label=\"Conversation\")\n",
    "        # Create a textbox for the user's message\n",
    "        message = gr.Textbox(label=\"Message\")\n",
    "        # Create a state object to store the conversation history\n",
    "        history_state = gr.State()\n",
    "        # Create a button to send the user's message\n",
    "        btn = gr.Button(value=\"Send\")\n",
    "\n",
    "        # Connect the send button to the chat function\n",
    "        btn.click(chat, inputs=[message, chatbot, history_state], outputs=[message, chatbot, history_state])\n",
    "        # Return the app\n",
    "        return app\n",
    "# ---------------------------------------------------------------------------------------        \n",
    "# Call the launch_chatbot function to start the chatbot interface using Gradio\n",
    "app = get_chatbot_app()\n",
    "app.queue()  # this is to be able to queue multiple requests at once\n",
    "app.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1cb134",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
